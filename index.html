<!doctype html>
<html lang="en">

<head>

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Undergraduate Researcher, NIT Rourkela">
    <meta name="author" content="Arnav Samal">
    <meta name="theme-color" content="#222222">

    <link rel="apple-touch-icon" sizes="180x180" href="images/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="images/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="images/favicon/favicon-16x16.png">
    <link rel="manifest" href="images/favicon/site.webmanifest">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
        integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="stylesheets/style.css">

    <title>Arnav Samal</title>
</head>

<body>
    <!-- <h1>Hello, world!</h1> -->
    <!-- <div id="dark-mode-toggle" style="position: fixed; top: 10px; right: 10px; cursor: pointer;">
        <img src="dark-mode-icon.png" alt="Toggle Dark Mode" width="30" height="30">
      </div> -->
    <div class="container pt-5 allstuffp">
        <div class="row pt-5 allstuff">
            <div class="col-md-4 pt-5">
                <div class="fixed-posi">
                    <p class="name"><span style="font-weight: 900;">Arnav</span> Samal</p>
                    <img src="imgs/profile2.jpg" class="profilepic pt-3 pb-2">
                    <!-- <div class="pt-5 menur"><a class="menulink" target="_blank"
                            href="assets/cv.pdf">curriculum vitae</a></div> -->
                    <div class=""><a class="menulink" target="_blank"
                            href="https://scholar.google.com/citations?user=6X4PnXgAAAAJ&hl=en">google scholar</a></div>
                    <div class=""><a class="menulink" target="_blank" href="mailto:samalarnav@gmail.com">email address</a></div>
                    <div class=""><a class="menulink" target="_blank" href="https://github.com/arnavsm">github profile</a></div>
                    <div class=""><a class="menulink" target="_blank"
                            href="https://www.linkedin.com/in/arnavsamal">linkedin</a></div>
                    <div class=""><a class="menulink" target="_blank"
                            href="https://kaggle.com/arnavs19">kaggle</a></div>


                    <!-- <div class="pt-5">usercontext</div> -->
                    <!-- <div class="">travel history</div> -->
                    <!-- <div class="">this template</div> -->
                </div>
            </div>
            <div class="col-md-8 pt-5 about">

                    <p>
                    I am an undergraduate student, majoring in computer science and engineering at the <a class="in-text" href="https://www.nitrkl.ac.in" target="_blank">National Institute of Technology Rourkela</a>, India. 

                    I am broadly interested in deep learning and machine learning research, with a focus on multi-modal learning and interpretability.
                    </p>
                    <p>
                    <!-- <b>Current Research</b><br> -->
                    Currently, I am working on continual learning and test-time training for Vision Transformers at the 
                    <a class="in-text" href="https://visual-computing.in" target="_blank">Visual Computing Lab (VCL)</a>, 
                    <a class="in-text" href="https://iisc.ac.in" target="_blank">IISc Bangalore</a>, under the guidance of 
                    <a class="in-text" href="https://anirbanchakraborty.github.io" target="_blank">Dr. Anirban Chakraborty</a>.
                    </p>

                    <p>
                    <!-- <b>Previous Research Experience</b><br> -->
                    Previously, in the summer of 2024, I worked with 
                    <a class="in-text" href="https://krmopuri.github.io" target="_blank">Dr. Konda Reddy Mopuri</a> at 
                    <a class="in-text" href="https://www.iith.ac.in" target="_blank">IIT Hyderabad</a> on Vision Transformers and explainability; 
                    in the summer of 2025, I worked with 
                    <a class="in-text" href="https://people.iith.ac.in/vineethnb/index.html" target="_blank">Prof. Vineeth Balasubramanian</a> on faithful Concept Bottleneck Models (CBMs) for medical imaging; 
                    and in the subsequent semester, I worked with 
                    <a class="in-text" href="https://www.nitrkl.ac.in/FacultyStaff/FacultyProfile/?id=deyp" target="_blank">Dr. Prasenjit Dey</a> on diffusion-based sketch-to-face synthesis.
                    </p>

                    For more details, 
                    <!-- refer to my <a class="in-text" href="assets/cv.pdf" target="_blank">CV</a> or  -->
                    drop me an <a class="in-text" href="mailto:samalarnav@gmail.com">email</a>.

                <p class="header pt-5">News & Honors</p>
                <ul class="list">
                        <li>Spending my final semester at <a class="in-text" href="https://iisc.ac.in" target="_blank">IISc Bangalore</a>, at the <a class="in-text" href="https://visual-computing.in" target="_blank">Visual Computing Lab (VCL)</a> under <a class="in-text" href="https://anirbanchakraborty.github.io" target="_blank">Dr. Anirban Chakraborty</a>!</li>

                        <li>Ranked <a class="in-text" href="https://drive.google.com/file/d/1q403e4t5Qt5GrzyZbt2fbu-ER4hEnxkT/view?usp=sharing" target="_blank">5th</a> in the Computer Science and Engineering department at NIT Rourkela (upto 7th semester).</li>

                        <li>Accepted into the highly competitive <a class="in-text" href="https://ikdd.acm.org/uplink-2025.php" target="_blank">ACM-IKDD Uplink 2025 program</a>, where I will be working with <a class="in-text" href="https://people.iith.ac.in/vineethnb/index.html" target="_blank">Prof. Vineeth Balasubramanian</a> on Reasoning in Vision-Language Models back at <a class="in-text" href="https://www.iith.ac.in" target="_blank">IIT Hyderabad</a>.</li>

                        <li>Accepted into the prestigious <a class="in-text" href="https://www.arlis.umd.edu/research-impact/research-intelligence-security-challenges-risc" target="_blank">RISC 2025 program</a>  at the <a class="in-text" href="https://umd.edu" target="_blank">University of Maryland</a>.</li>
    
                        <li>Ranked 5th in the <a class="in-text" href="https://misahub.in/cv2024.html" target="_blank">Capsule Vision Challenge 2024</a> as Team Seq2Cure, 
                            organized by <a class="in-text" href="https://cvip2024.iiitdm.ac.in/challenge" target="_blank">CVIP 2024</a>.</li>
                        
                        <li>Selected among 170 from 20,000+ applicants for the <a class="in-text" href="https://www.iith.ac.in/assets/files/pdf/Second-list-for-SURE-Internship-2024.pdf" target="_blank">
                            SURE program</a> at IIT Hyderabad.</li>
                        
                        <li>Achieved 2nd place in <a class="in-text" href="https://www.instagram.com/p/C5jGAjSvOiQ/?utm_source=ig_web_button_share_sheet&igsh=ZDNlZDc0MzIxNw==" target="_blank">HackFest 24</a>, a hackathon organized by <a class="in-text" href="https://www.linkedin.com/company/machine-learning-for-everyone-ml4e" target="_blank">ML4E</a> for undergraduate students.</li>
                        
                        <li>Recognized as a <a class="in-text" href="https://www.kaggle.com/arnavs19" target="_blank">Kaggle Expert</a> in Datasets & Notebooks.</li>
                </ul>

                <p class="header pt-5">Publications</p>
                <div class="py-2">
                    <p class="paper my-2 pl-2">
                        <span class="papertitle">CapsoNet: A CNN-Transformer Ensemble for Multi-Class Abnormality Detection in Video Capsule Endoscopy</span><br>
                        <span class="thisauthor">Arnav Samal</span>, Ranya Batsyas <br>
                        <span class="conf"><a class="confshort" href="https://arxiv.org">arXiv preprint</a> | 
                            Oct, 2024 </span><br>
                        <a class="tag" href="https://arxiv.org/pdf/2410.18879" target="_blank">pdf</a><span
                            class="tagsep"> |</span>
                        <a class="tag" href="https://arxiv.org/abs/2410.18879" target="_blank">abstract</a><span
                            class="tagsep"> |</span>
                        <a class="tag" href="https://github.com/arnavsm/capsule-vision-2024"
                            target="_blank">code</a><span class="tagsep"></span> 
                        <!-- <a class="tag" href="cites/uai2024.bib" target="_blank">bibtex</a><br> -->
                </p>
                </div>

                
                <p class="header pt-5">Selected Projects</p>

                <div class="py-2">
                    <p class="paper my-2 pl-2">
                        <span class="papertitle">SketchWarp</span><br>
                        <span class="conf">Developed a self-supervised learning framework in PyTorch for dense photo-to-sketch correspondences, enabling automatic image-to-sketch warping. Designed and implemented training and evaluation pipelines inspired by the “Learning Dense Correspondences between Photos and Sketches” paper.</span><br>
                        <a class="tag" href="https://github.com/arnavsm/sketch-warp" target="_blank">code</a>
                        <span class="tagsep"> |</span>
                        <a class="tag" href="https://arxiv.org/pdf/2307.12967" target="_blank">paper</a><br>
                </p>
                </div>

                <div class="py-2">
                    <p class="paper my-2 pl-2">
                        <span class="papertitle">NeurIPS Ariel Data Challenge 2024</span><br>
                        <span class="conf">Developed a pipeline for predicting spectral values in the NeurIPS Ariel Data Challenge 2024 using time-series calibration, spatial aggregation, and gradient-based phase detection. 
                            Ranked 257/1,152 by applying Nelder-Mead optimization and cubic polynomial fitting to model planetary transits from raw sensor data.</span><br>
                        <a class="tag" href="https://github.com/arnavsm/neurips-ariel-2024"
                            target="_blank">code</a><span class="tagsep"> |</span>
                        <a class="tag" href="https://www.kaggle.com/competitions/ariel-data-challenge-2024" target="_blank">kaggle</a><br>
                </p>
                </div>

                <div class="py-2">
                    <p class="paper my-2 pl-2">
                        <span class="papertitle">Paper Implementations</span><br>
                        <span class="conf">Implemented significant AI and machine learning research papers, including transformers (such as GPT variants, BERT, ViTs) as well as LoRA and neural style transfer.
                            I actively implement new papers and continuously update this repository.
                        </span><br>
                        <a class="tag" href="https://github.com/arnavsm/paper-implementations" target="_blank">code</a><br>
                </p>
                </div>

                <div class="py-2">
                    <p class="paper my-2 pl-2">
                        <span class="papertitle">Measuring Patch Importance in ViT's (Vanilla & Attention Rollout)</span><br>
                        <span class="conf">Analyzed patch importance in Vision Transformers using attention scores of the [CLS] token across MHSA mechanims in all blocks, visualizing the distribution of top-k patch tokens. 
                            Implemented Attention Rollout to propagate attention through layers, creating interpretable visualizations of information flow and enhancing understanding of self-attention mechanisms.
                        </span><br>
                        <a class="tag" href="https://github.com/arnavsm/vit-patch-importance" target="_blank">code</a><br>
                </p>
                </div>

                <!-- <div class="py-2">
                    <p class="paper my-2 pl-2">
                        <span class="papertitle">DeBERTa-ELL</span><br>
                        <span class="conf">Developed an NLP model using DeBERTa-v3 to assess English proficiency in high school essays, evaluating cohesion, syntax, and grammar. 
                            Achieved a final MCRMSE score of 0.4566 through full parameter fine-tuning and multi-label stratified k-fold cross-validation.</span><br>
                        <a class="tag" href="https://github.com/arnavsm/deberta-ell" target="_blank">code</a><br>
                </p>
                </div> -->



                <div class="row text-center py-4">

                    <div class="mx-auto mt-2">
                        <img class="img-fluid instilogo p-1" src="imgs/logos/iisc.png">
                        <div class="institution">IISc Bangalore</div>
                        <div class="years">2026</div>
                    </div>

                    <!-- <div class="mx-auto mt-2">
                        <img class="img-fluid instilogo p-1" src="imgs/logos/lab1055.jpg">
                        <div class="institution">Lab1055</div>
                        <div class="years">2025</div>
                    </div> -->

                    <!-- <div class="mx-auto mt-2">
                        <img class="img-fluid instilogo p-1" src="imgs/logos/ikdd.jpg">
                        <div class="institution">ACM-IKDD</div>
                        <div class="years">S2025</div>
                    </div> -->

                    <div class="mx-auto mt-2">
                        <img class="img-fluid instilogo p-1" src="imgs/logos/respai.png">
                        <div class="institution">RespAI Lab</div>
                        <div class="years">2025</div>
                    </div>

                    <div class="mx-auto mt-2">
                        <img class="img-fluid instilogo p-1" src="imgs/logos/iith.png">
                        <div class="institution">IIT Hyderabad</div>
                        <div class="years">S2024 & S2025</div>
                    </div>

                    <!-- <div class="mx-auto mt-2">
                        <img class="img-fluid instilogo p-1" src="imgs/logos/dil.png">
                        <div class="institution">DiL Lab</div>
                        <div class="years">2024</div>
                    </div> -->

                    <div class="mx-auto mt-2">
                        <img class="img-fluid instilogo p-1" src="imgs/logos/nitrkl.png">
                        <div class="institution">NIT Rourkela</div>
                        <div class="years">2022 - Present</div>
                    </div>

                </div>
                <br>
            </br>
            </div>

        </div>
    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
        integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous">
    </script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous">
    </script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
        integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous">
    </script>
    <script src="stylesheets/style.js"></script>
</body>
<style>
</style>

</html>
