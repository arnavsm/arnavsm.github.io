<!doctype html>
<html lang="en">

<head>

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Undergraduate Researcher, NIT Rourkela">
    <meta name="author" content="Arnav Samal">
    <meta name="theme-color" content="#222222">

    <link rel="apple-touch-icon" sizes="180x180" href="images/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="images/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="images/favicon/favicon-16x16.png">
    <link rel="manifest" href="images/favicon/site.webmanifest">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
        integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="stylesheets/style.css">

    <title>Arnav Samal</title>
</head>

<body>
    <!-- <h1>Hello, world!</h1> -->
    <!-- <div id="dark-mode-toggle" style="position: fixed; top: 10px; right: 10px; cursor: pointer;">
        <img src="dark-mode-icon.png" alt="Toggle Dark Mode" width="30" height="30">
      </div> -->
    <div class="container pt-4 allstuffp">
        <div class="row pt-4 allstuff">
            <div class="col-md-4 pt-5">
                <div class="fixed-posi">
                    <p class="name"><span style="font-weight: 900;">Arnav</span> Samal</p>
                    <img src="images/profile1.jpg" class="profilepic pt-3 pb-2">
                    <div class="pt-5 menur"><a class="menulink" target="_blank"
                            href="assets/resume.pdf">resume</a></div>
                    <!-- <div class=""><a class="menulink" target="_blank"
                            href="https://scholar.google.com/citations?user=6X4PnXgAAAAJ&hl=en">google scholar</a></div> -->
                    <div class=""><a class="menulink" target="_blank" href="mailto:samalarnav@gmail.com">email</a></div>
                    <div class=""><a class="menulink" target="_blank" href="https://github.com/arnavsm">github</a></div>
                    <div class=""><a class="menulink" target="_blank"
                            href="https://www.linkedin.com/in/arnavsamal">linkedin</a></div>
                    <div class=""><a class="menulink" target="_blank"
                            href="https://kaggle.com/arnavs19">kaggle</a></div>


                    <!-- <div class="pt-5">usercontext</div> -->
                    <!-- <div class="">travel history</div> -->
                    <!-- <div class="">this template</div> -->
                </div>
            </div>
            <div class="col-md-8 pt-5 about">
                    I am an undergraduate student, majoring in computer science and engineering at the <a class="in-text"
                    href="https://www.nitrkl.ac.in" target="_blank">National Institute of Technology Rourkela</a>, India. 
                    I am broadly interested in algorithmic research in deep learning and machine learning, specifically multi-modal learning and interpretability.
                    My current work, under the guidance of <a class="in-text" href="https://murarimandal.github.io" target="_blank">Dr. Murari Mandal</a>, 
                    focuses on large language models (LLMs), exploring inference optimization, model interpretability and alignment, and post-training RL techniques at the <a class="in-text" href="https://respailab.github.io" target="_blank">RespAI Lab</a>.

                    <br><br> 
                
                    Previously, I had the privilege of spending a wonderful summer as a <a class="in-text" href="https://www.iith.ac.in/news/2024/02/15/Summer-Undergraduate-Research-Exposure"
                    target="_blank">Research Intern</a> at the <a class="in-text" href="https://www.iith.ac.in" target="_blank">Indian Institute of Technology, Hyderabad</a>, India, under the guidance of <a class="in-text"
                    href="https://krmopuri.github.io" target="_blank">Dr. Konda Reddy Mopuri</a>.
                    During this time, and in the subsequent months of remote collaboration, I conducted research on Explainability in Vision Transformers. 
                    My work involved analyzing post-hoc explanation techniques and exploring token pruning methods to enhance the interpretability of image classification tasks.

                    <br><br> 

                    For more details, refer to my <a class="in-text" href="assets/resume.pdf" target="_blank">resume</a> or drop me an <a class="in-text" href="mailto:samalarnav@gmail.com">email</a>.

                <p class="header pt-5">News & Honors</p>
                <ul class="list">
                        <li>Accepted into the prestigious <a class="in-text" href="https://www.arlis.umd.edu/research-impact/research-intelligence-security-challenges-risc" target="_blank">Research for Intelligence & Security Challenges (RISC)</a> 2025 program at the <a class="in-text" href="https://umd.edu" target="_blank">University of Maryland</a>.</li>
    
                        <li>Ranked <a class="in-text" href="https://drive.google.com/file/d/1Qm-6zhbzVyJv4GX4skZCHDJ8GD0F0-9F/view?usp=sharing" target="_blank">12th</a> in the Computer Science and Engineering department at NIT Rourkela.</li>
    
                        <li>Ranked 5th in the <a class="in-text" href="https://misahub.in/cv2024.html" target="_blank">Capsule Vision Challenge 2024</a> as Team Seq2Cure, 
                            organized by <a class="in-text" href="https://cvip2024.iiitdm.ac.in/challenge" target="_blank">CVIP 2024</a>.</li>
                        
                        <li>Selected among 170 from 20,000+ applicants for the <a class="in-text" href="https://www.iith.ac.in/assets/files/pdf/Second-list-for-SURE-Internship-2024.pdf" target="_blank">
                            SURE program</a> at IIT Hyderabad.</li>
                        
                        <li>Achieved 2nd place in <a class="in-text" href="https://www.instagram.com/p/C5jGAjSvOiQ/?utm_source=ig_web_button_share_sheet&igsh=ZDNlZDc0MzIxNw==" target="_blank">HackFest 24</a>, a hackathon organized by <a class="in-text" href="https://www.linkedin.com/company/machine-learning-for-everyone-ml4e" target="_blank">ML4E</a> for undergraduate students.</li>
                        
                        <li>Recognized as a <a class="in-text" href="https://www.kaggle.com/arnavs19" target="_blank">Kaggle Expert</a> in Datasets & Notebooks.</li>
                </ul>

                <!-- <p class="header pt-5">Publications</p>
                <div class="py-2">
                    <p class="paper my-2 pl-2">
                        <span class="papertitle">Multi-Class Abnormality Classification in Video Capsule Endoscopy Using Deep Learning</span><br>
                        <span class="thisauthor">Arnav Samal</span>, Ranya Batsyas <br>
                        <span class="conf"><a class="confshort" href="https://arxiv.org/">arXiv preprint</a> | 
                            Oct, 2024 </span><br>
                        <a class="tag" href="https://arxiv.org/pdf/2410.18879" target="_blank">pdf</a><span
                            class="tagsep"> |</span>
                        <a class="tag" href="https://arxiv.org/abs/2410.18879" target="_blank">abstract</a><span
                            class="tagsep"> |</span>
                        <a class="tag" href="https://github.com/arnavsm/capsule-vision-2024"
                            target="_blank">code</a><span class="tagsep"></span> -->
                        <!-- <a class="tag" href="cites/uai2024.bib" target="_blank">bibtex</a><br> -->
                <!-- </p> -->
                <!-- </div> -->

                
                <p class="header pt-5">Selected Projects</p>

                <div class="py-2">
                    <p class="paper my-2 pl-2">
                        <span class="papertitle">SketchWarp <em>(ongoing)</em></span><br>
                        <span class="conf">Developed a self-supervised learning framework in PyTorch for dense photo-to-sketch correspondences, enabling automatic image-to-sketch warping. Designed and implemented training and evaluation pipelines inspired by the “Learning Dense Correspondences between Photos and Sketches” paper.</span><br>
                        <a class="tag" href="https://github.com/arnavsm/sketch-warp" target="_blank">code</a>
                        <span class="tagsep"> |</span>
                        <a class="tag" href="https://arxiv.org/pdf/2307.12967" target="_blank">paper</a><br>
                </p>
                </div>

                <div class="py-2">
                    <p class="paper my-2 pl-2">
                        <span class="papertitle">NeurIPS Ariel Data Challenge 2024</span><br>
                        <span class="conf">Developed a pipeline for predicting spectral values in the NeurIPS Ariel Data Challenge 2024 using time-series calibration, spatial aggregation, and gradient-based phase detection. 
                            Ranked 257/1,152 by applying Nelder-Mead optimization and cubic polynomial fitting to model planetary transits from raw sensor data.</span><br>
                        <a class="tag" href="https://github.com/arnavsm/neurips-ariel-2024"
                            target="_blank">code</a><span class="tagsep"> |</span>
                        <a class="tag" href="https://www.kaggle.com/competitions/ariel-data-challenge-2024" target="_blank">kaggle</a><br>
                </p>
                </div>

                <div class="py-2">
                    <p class="paper my-2 pl-2">
                        <span class="papertitle">Paper Implementations</span><br>
                        <span class="conf">Implemented significant AI and machine learning research papers, including transformers—GPT variants, BERT, Vision Transformers (ViT)—as well as LoRA and neural style transfer.
                            I actively implement new papers and continuously update this repository.
                        </span><br>
                        <a class="tag" href="https://github.com/arnavsm/paper-implementations" target="_blank">code</a><br>
                </p>
                </div>

                <div class="py-2">
                    <p class="paper my-2 pl-2">
                        <span class="papertitle">Measuring Patch Importance in ViT's (Vanilla & Attention Rollout)</span><br>
                        <span class="conf">Analyzed patch importance in Vision Transformers using attention scores of the [CLS] token across MHSA mechanims in all blocks, visualizing the distribution of top-k patch tokens. 
                            Implemented Attention Rollout to propagate attention through layers, creating interpretable visualizations of information flow and enhancing understanding of self-attention mechanisms.
                        </span><br>
                        <a class="tag" href="https://github.com/arnavsm/vit-patch-importance" target="_blank">code</a><br>
                </p>
                </div>

                <!-- <div class="py-2">
                    <p class="paper my-2 pl-2">
                        <span class="papertitle">DeBERTa-ELL</span><br>
                        <span class="conf">Developed an NLP model using DeBERTa-v3 to assess English proficiency in high school essays, evaluating cohesion, syntax, and grammar. 
                            Achieved a final MCRMSE score of 0.4566 through full parameter fine-tuning and multi-label stratified k-fold cross-validation.</span><br>
                        <a class="tag" href="https://github.com/arnavsm/deberta-ell" target="_blank">code</a><br>
                </p>
                </div> -->


                


                <div class="row text-center py-4">

                    <div class="mx-auto mt-2">
                        <img class="img-fluid instilogo p-1" src="images/logos/respai.png">
                        <div class="institution">RespAI Lab</div>
                        <div class="years">Mar. 2025 - Present</div>
                    </div>

                    <div class="mx-auto mt-2">
                        <img class="img-fluid instilogo p-1" src="images/logos/iith.png">
                        <div class="institution">IIT Hyderabad</div>
                        <div class="years">May 2024 - Sept. 2024</div>
                    </div>

                    <div class="mx-auto mt-2">
                        <img class="img-fluid instilogo p-1" src="images/logos/nitrkl.png">
                        <div class="institution">NIT Rourkela</div>
                        <div class="years">Nov. 2022 - Present</div>
                    </div>

                </div>
                <br>
            </br>
            </div>

        </div>
    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
        integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous">
    </script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous">
    </script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
        integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous">
    </script>
    <script src="stylesheets/style.js"></script>
</body>
<style>
</style>

</html>
